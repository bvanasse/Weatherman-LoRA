# Weatherman-LoRA Mac M4 Training Environment Dependencies
# For Mac M4 with MPS backend and 32GB unified memory
# Python 3.10+ required for compatibility
# Base Model: Mistral 7B Instruct v0.2

# ============================================================
# PyTorch with MPS Support
# ============================================================
# Install PyTorch 2.1+ with MPS backend support
# MPS (Metal Performance Shaders) provides GPU acceleration on Apple Silicon
torch>=2.1.0
torchvision>=0.16.0
torchaudio>=2.1.0

# ============================================================
# Core Training Libraries
# ============================================================
# HuggingFace Transformers for Mistral 7B Instruct
# Pin 4.36.0 for Mistral 7B Instruct compatibility
transformers==4.36.0

# PEFT: Parameter-Efficient Fine-Tuning (LoRA)
# Pin 0.7.0 for stable LoRA configuration
peft==0.7.0

# TRL: Transformer Reinforcement Learning (SFTTrainer)
# Pin 0.7.4 for supervised fine-tuning with chat format
trl==0.7.4

# Accelerate: Training coordination and device management
# Pin 0.25.0 for MPS backend support
accelerate==0.25.0

# bitsandbytes: 4-bit quantization for QLoRA
# Pin 0.41.0 for NF4 quantization with double quantization
# Note: MPS-compatible version
bitsandbytes==0.41.0

# HuggingFace Datasets library
# Pin 2.15.0 for JSONL data loading
datasets==2.15.0

# ============================================================
# Data Processing Libraries
# ============================================================
# Data manipulation
pandas==2.1.3

# JSONL file handling
jsonlines==4.0.0

# ============================================================
# Training Monitoring
# ============================================================
# Weights & Biases for experiment tracking
wandb==0.16.1

# ============================================================
# Utilities
# ============================================================
# Scientific computing
scipy==1.11.4
scikit-learn==1.3.2

# HuggingFace Hub for model downloads
huggingface-hub==0.19.4

# Tokenization
sentencepiece==0.1.99

# Model serialization
protobuf==3.20.3

# ============================================================
# Testing
# ============================================================
# Testing framework
pytest==7.4.3

# ============================================================
# Notes
# ============================================================
# Platform: Mac M4 with MPS backend
# Base Model: Mistral 7B Instruct v0.2
# Unified Memory: 32GB (shared between CPU and GPU)
#
# Key Differences from H100 Environment:
# - MPS backend instead of CUDA
# - No Flash Attention 2 (CUDA-only)
# - Reduced sequence length: 2048 tokens (vs 4096 on H100)
# - Smaller batch size: 1-2 (vs 4-8 on H100)
# - Higher gradient accumulation: 16 (vs 4 on H100)
#
# Expected Training Time: 8-12 hours for 3 epochs (15K examples)
#   - 2-3x slower than H100 due to:
#     - Smaller batch size (more gradient steps)
#     - No Flash Attention (O(N^2) vs O(N) memory)
#     - MPS vs CUDA performance difference
#
# Memory Budget (32GB Unified):
#   - Base model (4-bit): ~5-6GB
#   - Activations (2048 seq, batch 1-2): ~12-15GB
#   - Optimizer states: ~3-4GB
#   - Gradients: ~4-6GB
#   - System overhead: ~4-8GB (macOS)
#   - Total: ~24-28GB peak usage
#
# Installation:
#   python3.10 -m venv .venv-m4
#   source .venv-m4/bin/activate
#   pip install -r requirements-m4.txt
#
# Verify MPS:
#   python -c "import torch; print(torch.backends.mps.is_available())"
#
# Performance Tips:
#   - Close all unnecessary applications
#   - Monitor Activity Monitor for memory pressure
#   - Train overnight for 8-12 hour duration
#   - Use wandb for remote monitoring
