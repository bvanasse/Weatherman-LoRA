# Weatherman-LoRA Axolotl Configuration for Mac M4
# Optimized for Mac M4 with MPS backend and 32GB unified memory
# Base Model: Mistral 7B Instruct v0.3

# ============================================================
# Base Model Configuration
# ============================================================
base_model: mistralai/Mistral-7B-Instruct-v0.3
model_type: MistralForCausalLM
tokenizer_type: AutoTokenizer
trust_remote_code: true

# ============================================================
# Dataset Configuration
# ============================================================
# Using diverse training data (regenerated to remove repetitive templates)
# - Original data had 59% template repetition
# - Diverse data has <3% templates for better generalization
# - See DATA_REGENERATION_GUIDE.md for details

datasets:
  - path: data/synthetic/final_train_diverse.jsonl
    ds_type: json  # JSONL format
    type: chat_template  # Use chat template format (supports tool calls)
    field_messages: messages  # Field containing message array
    message_field_role: role
    message_field_content: content

# Validation Dataset
val_set_size: 0  # We use a separate validation file
test_datasets:
  - path: data/synthetic/final_validation_diverse.jsonl
    ds_type: json
    type: chat_template
    field_messages: messages
    message_field_role: role
    message_field_content: content

# ============================================================
# Chat Template
# ============================================================
# Use Mistral's chat template for proper formatting
chat_template: mistral

# ============================================================
# LoRA Configuration
# ============================================================
adapter: lora
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj
lora_fan_in_fan_out: false

# ============================================================
# Quantization (QLoRA)
# ============================================================
# DISABLED: bitsandbytes not available on macOS
# Using full precision LoRA instead
load_in_4bit: false
load_in_8bit: false

# ============================================================
# Sequence and Packing Configuration
# ============================================================
sequence_len: 2048  # Reduced for MPS memory constraints
sample_packing: false  # Disable for tool calling to preserve conversation boundaries
pad_to_sequence_len: true

# ============================================================
# Training Hyperparameters (M4 Optimized)
# ============================================================
# Using full precision (no quantization) - requires smaller batch
micro_batch_size: 1  # Small batch for MPS memory
gradient_accumulation_steps: 32  # Higher accumulation without quantization
num_epochs: 3
learning_rate: 0.0002
lr_scheduler: cosine
warmup_steps: 100

# ============================================================
# Optimization
# ============================================================
optimizer: adamw_torch
weight_decay: 0.01
max_grad_norm: 1.0

# ============================================================
# Mixed Precision (MPS Compatible)
# ============================================================
bf16: false  # MPS doesn't support bfloat16 well
fp16: true   # Use fp16 for MPS
tf32: false  # Not available on MPS

# ============================================================
# Gradient Checkpointing
# ============================================================
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false

# ============================================================
# Attention Optimization (MPS Compatible)
# ============================================================
flash_attention: false  # Flash Attention is CUDA-only
xformers_attention: false  # xFormers not compatible with MPS
s2_attention: false

# ============================================================
# Logging and Checkpointing
# ============================================================
output_dir: ./adapters/weatherman-lora-axolotl-m4
save_strategy: steps
save_steps: 1000  # Less frequent saves for slower training
save_total_limit: 2  # Keep only 2 checkpoints to save space
logging_steps: 20
eval_steps: 1000
eval_strategy: steps

# ============================================================
# Weights & Biases Integration
# ============================================================
wandb_project: weatherman-lora
wandb_run_id: weatherman-axolotl-m4
wandb_log_model: false
wandb_watch: false

# ============================================================
# HuggingFace Hub (optional)
# ============================================================
# Uncomment to push to HuggingFace Hub
# hub_model_id: your-username/weatherman-lora-m4
# push_dataset_to_hub: false
# hub_strategy: checkpoint

# ============================================================
# Device Configuration (MPS)
# ============================================================
local_rank:
device: mps  # Use Metal Performance Shaders on M4
device_map:  # Empty for MPS (no auto mapping)

# ============================================================
# Special Tokens (Mistral v0.3)
# ============================================================
special_tokens:
  pad_token: "<pad>"
  bos_token: "<s>"
  eos_token: "</s>"
  unk_token: "<unk>"

# ============================================================
# Training Efficiency Settings
# ============================================================
# Early stopping (optional)
early_stopping_patience: 3

# Resume from checkpoint
resume_from_checkpoint:

# ============================================================
# Data Processing
# ============================================================
# Disable sample packing for tool calling format
strict: false

# ============================================================
# Notes
# ============================================================
# Expected Training Time: 8-12 hours on M4 for 3 epochs
# Memory Usage: ~24-28GB unified memory with QLoRA + gradient checkpointing
# Dataset: 14,399 training examples, 1,601 validation examples
#
# Training Progress:
# - Total steps per epoch: ~900 (14,399 / (1 * 16))
# - Total steps: ~2,700 (3 epochs)
# - Checkpoints saved every 1000 steps
# - Evaluation every 1000 steps
#
# Performance Notes:
# - MPS is 2-3x slower than CUDA (H100)
# - Smaller batch size requires more gradient steps
# - No Flash Attention (O(N^2) vs O(N) memory complexity)
# - Recommended: Train overnight or over weekend
#
# Monitor via:
# - Weights & Biases dashboard
# - Local logs in output_dir/logs/
# - Activity Monitor for memory pressure
#
# Memory Management:
# - Close all unnecessary applications during training
# - Disable browser tabs and heavy apps
# - Monitor Activity Monitor for memory pressure warnings
