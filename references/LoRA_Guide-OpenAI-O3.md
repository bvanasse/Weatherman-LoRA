# LoRA Fine-Tuning Guide ‚Äì "TwainBot" Humor & Weather Specialist

> Fine-tune a compact LLM with LoRA so it speaks like Mark Twain, cracks Onion-style jokes, and can call a weather API. Designed for a single developer + AI coding agents over a 3-day weekend.

---
## üìù TL;DR
1. **Day 1 ‚Äì Data Harvest**: Collect corpora (The Onion, r/NotTheOnion, Mark Twain, Ben Franklin, weather jokes, Open-Weather API docs & example calls). Target ‚âà100 k lines raw.
2. **Day 2 ‚Äì Data Refinement**: Clean, deduplicate, tokenize, convert to chat-style JSONL; auto-label tool-call examples; split train/val.
3. **Day 3 ‚Äì LoRA Training & Eval**: LoRA-adapt a 7 B base (e.g. Mistral-7B-Instruct v0.2). Tune for 3-4 h on single A100 / 24 h on consumer GPU. Evaluate with custom humor+API benches. Ship the `.pt` adapter + usage README.

Outcome: An LLM that answers like Twain, jokes about weather, and returns JSON weather API calls when asked.

---
## 1. Project Goals
‚Ä¢ Demonstrate end-to-end data stewardship & LoRA impact.
‚Ä¢ Produce two adapters:
  a. **Style-Only** ‚Äì author humor corpus.
  b. **Style + Tool** ‚Äì corpus + function-call exemplars.
‚Ä¢ Compare output quality & hallucination rate.

---
## 2. Phased 3-Day Weekend Plan
### Evening Before (¬Ω h)
1. Install tooling (`conda`, `torch`, `transformers`, `peft`, `datasets`, `trafilatura`, `beautifulsoup4`, `playwright`, `pydantic`).
2. Fork this repo; create `data/`, `scripts/`, `notebooks/`.

### Day 1 ‚Äì Data Harvest (6 h)
Task | Tooling | Notes
--- | --- | ---
Scrape The Onion site (RSS/plaid-bytes) | `playwright` + `trafilatura` | Target ‚â•10 k articles.
Pull r/TheOnion & r/NotTheOnion comments | Pushshift, `praw` | Humor in social context.
Download Mark Twain corpus | Project Gutenberg API | Filter essays, speeches.
Download Ben Franklin writings | Gutenberg | Letters & aphorisms.
Gather weather jokes | Kaggle "short-jokes", regex filter ‚Äúweather‚Äù | Augment via GPT paraphrasing.
Collect weather API docs & examples | cURL scrape | Later used to craft tool-call demonstrations.

Deliverable: `raw/` folder with source-labeled `.txt` files and metadata CSV.

### Day 2 ‚Äì Data Refinement (6 h)
1. **Cleaning Pipeline** (`scripts/clean.py`):
   ‚Ä¢ Strip HTML, ads, stage directions.
   ‚Ä¢ Remove Gutenberg headers/footers.
   ‚Ä¢ Language detect ‚Üí keep English.
   ‚Ä¢ Drop duplicates with MinHash.
2. **Segmentation**:
   ‚Ä¢ Split humor pieces into joke units (newline/new-sentence heuristics).
   ‚Ä¢ Split prose into ‚â§512-token chunks, keep paragraph boundaries.
3. **Conversion to Chat JSONL**:
   ‚Ä¢ For style data: `{ "messages": [{"role":"user","content":"Say something about the mississippi."}, {"role":"assistant","content":"<Twain-style response>"}] }`
   ‚Ä¢ For API data: include `"tool": {"name":"getWeather", "arguments":{...}}` messages.
4. **Label Generation**:
   ‚Ä¢ Auto-generate user prompts with GPT-4o for each chunk (5¬Ωc/1k).
   ‚Ä¢ Validate with regex & script QA.
5. **Split**: 90 % train / 10 % val.

### Day 3 ‚Äì LoRA Training & Evaluation (6 h)
1. **Select Base**: `mistralai/Mistral-7B-Instruct-v0.2` (Apache 2.0).
2. **Config** (`scripts/train_lora.py`)
   ‚Ä¢ r = 64, Œ± = 16, dropout = 0.05.
   ‚Ä¢ Target modules: `q_proj`, `v_proj`, `k_proj`, `o_proj`.
   ‚Ä¢ Batch = 128 seq/8 grad acc, lr = 2e-4, cosine-decay.
3. **Train**: `accelerate launch ‚Ä¶` (‚âà3 h on A100 80 GB; set epochs = 1, steps ‚âà 800).
4. **Eval**:
   ‚Ä¢ Automated humor test set (punchline detection BLEU, HUMO score).
   ‚Ä¢ Tool-call correctness (JSON schema validation).
   ‚Ä¢ Manual QA: 30 prompts side-by-side (baseline vs adapters).
5. **Package**: Save adapter weights `.safetensors`, push to Hugging Face, update README with usage snippet.

---
## 3. Data Generation & Augmentation Techniques
### 3.1 Parsing Tricks
‚Ä¢ `trafilatura` for readability extraction.
‚Ä¢ Use GPT-4o to rewrite headlines into prompts.
‚Ä¢ Deduplicate with `datasketch` MinHashLSH.

### 3.2 Synthetic Data
Method | Purpose | Example
--- | --- | ---
Prompt Paraphrasing | Increase style variety | "Rewrite this joke in 3 ways keeping Twain tone."
Topic Injection | Ensure weather coverage | "Write a Twain-style quip about humidity in July."
Tool-Call Synthesis | Teach JSON calls | Provide system + user instruct; generate assistant JSON.

### 3.3 Safety Filters
‚Ä¢ `openai-moderation` / `together-ai` filters before final dataset.

---
## 4. Pre-training vs Fine-Tuning Rationale
‚Ä¢ Full pre-training is GPU-week intense ‚Üí We reuse a strong base.
‚Ä¢ LoRA allows weekend-scale adaptation (<5 % parameters) and adapter swapping.
‚Ä¢ Separate adapters enable ablation (style-only vs style+tool).

---
## 5. Expected Outcomes & Metrics
Metric | Target
--- | ---
Humor perplexity ‚Üì | 10 % over base on validation
Joke preference (human, n=30) | ‚â•70 % prefer adapter
JSON tool-call validity | ‚â•95 % parses with `json.loads`

---
## 6. Resources & Further Reading
‚Ä¢ LoRA paper ‚Äì Hu et al., 2021  
‚Ä¢ Project Gutenberg ‚Äì https://www.gutenberg.org  
‚Ä¢ The Onion RSS ‚Äì https://www.theonion.com/rss  
‚Ä¢ Pushshift Reddit API ‚Äì https://github.com/pushshift/api  
‚Ä¢ Mistral-7B-Instruct v0.2 ‚Äì https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2  
‚Ä¢ PEFT ‚Äì https://github.com/huggingface/peft  
‚Ä¢ trafilatura (web scraping) ‚Äì https://github.com/adbar/trafilatura  
‚Ä¢ HUMO humor metric ‚Äì https://arxiv.org/abs/2305.06929

---
### Maintain & Iterate
After initial weekend: collect user feedback, append to dataset, re-train for incremental gains.

---
¬© 2025 TwainBot Labs
